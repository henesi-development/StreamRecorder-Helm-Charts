apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: streamrecorder-etl
  namespace: stream-recorder
spec:
  serviceAccountName: argo-workflow-sa
  entrypoint: etl-sequence
  ttlStrategy:
    secondsAfterCompletion: 86400   # delete 1 day after completion
    secondsAfterSuccess: 3600       # delete 1 hour after success
    secondsAfterFailure: 7200       # delete 2 hours after failure

  arguments:
    parameters:
      - name: ETL_DAYS_BACK
        value: "1"

  templates:
    # Main sequence
    - name: etl-sequence
      steps:
        - - name: recording-export
            template: run-spark-job
            arguments:
              parameters:
                - name: SPARK_MAIN_CLASS
                  value: "com.henesi.etl.app.RecordingExport"
                - name: ETL_DAYS_BACK
                  value: "{{workflow.parameters.ETL_DAYS_BACK}}"
        - - name: recording-report
            template: run-spark-job
            arguments:
              parameters:
                - name: SPARK_MAIN_CLASS
                  value: "com.henesi.etl.app.RecordingReport"
                - name: ETL_DAYS_BACK
                  value: "{{workflow.parameters.ETL_DAYS_BACK}}"

    # Spark job template (shared by both export & report)
    - name: run-spark-job
      inputs:
        parameters:
          - name: SPARK_MAIN_CLASS
          - name: ETL_DAYS_BACK
      container:
        image: gcieplechowicz/streamrecorder-etl:0.0.11
        imagePullPolicy: IfNotPresent
        env:
          - name: SPARK_MAIN_CLASS
            value: "{{inputs.parameters.SPARK_MAIN_CLASS}}"
        command: []
        args:
          - "--conf"
          - "spark.driver.extraJavaOptions=-Detl.daysBack={{inputs.parameters.ETL_DAYS_BACK}}"
        resources:
          requests:
            memory: "256Mi"
            cpu: "0.25"
          limits:
            memory: "512Mi"
            cpu: "0.5"